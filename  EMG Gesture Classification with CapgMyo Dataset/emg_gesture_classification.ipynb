{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy scipy tensorflow scikit-learn pywavelets tf2onnx matplotlib pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import scipy.io as sio\n",
    "from scipy.signal import butter, filtfilt, welch\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tf2onnx\n",
    "import logging\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "import zipfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CapgMyoDataLoader:\n",
    "    def __init__(self, data_dir: str, window_size: int = 1000, overlap: float = 0.5, fs: int = 1000):\n",
    "        self.data_dir = data_dir\n",
    "        self.window_size = window_size\n",
    "        self.overlap = overlap\n",
    "        self.fs = fs\n",
    "        self.stride = int(window_size * (1 - overlap))\n",
    "        self.num_channels = 8\n",
    "        self.num_gestures = 8\n",
    "\n",
    "    def download_dataset(self):\n",
    "        \"\"\"Download CapgMyo dataset if not already present\"\"\"\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir, exist_ok=True)\n",
    "            # Note: Replace with actual dataset URL\n",
    "            url = \"https://example.com/capgmyo_dataset.zip\"\n",
    "            logger.info(\"Downloading CapgMyo dataset...\")\n",
    "            urllib.request.urlretrieve(url, \"capgmyo_dataset.zip\")\n",
    "            \n",
    "            with zipfile.ZipFile(\"capgmyo_dataset.zip\", 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.data_dir)\n",
    "            os.remove(\"capgmyo_dataset.zip\")\n",
    "            logger.info(\"Dataset downloaded and extracted successfully\")\n",
    "\n",
    "    def load_data(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load and preprocess CapgMyo dataset\"\"\"\n",
    "        logger.info(\"Loading CapgMyo dataset...\")\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for subject_dir in sorted(os.listdir(self.data_dir)):\n",
    "            if not subject_dir.startswith('subject'):\n",
    "                continue\n",
    "                \n",
    "            subject_path = os.path.join(self.data_dir, subject_dir)\n",
    "            logger.info(f\"Processing {subject_dir}\")\n",
    "            \n",
    "            for session in ['session1', 'session2']:\n",
    "                session_path = os.path.join(subject_path, session)\n",
    "                if not os.path.exists(session_path):\n",
    "                    continue\n",
    "                \n",
    "                for gesture in range(self.num_gestures):\n",
    "                    gesture_file = f'gesture{gesture+1}.mat'\n",
    "                    file_path = os.path.join(session_path, gesture_file)\n",
    "                    \n",
    "                    if not os.path.exists(file_path):\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        data = sio.loadmat(file_path)\n",
    "                        emg_data = data['emg']\n",
    "                        X.append(emg_data)\n",
    "                        y.extend([gesture] * len(emg_data))\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error loading {file_path}: {str(e)}\")\n",
    "        \n",
    "        if not X:\n",
    "            raise ValueError(\"No data was loaded. Please check the data directory path.\")\n",
    "        \n",
    "        X = np.vstack(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        logger.info(f\"Loaded dataset shape: {X.shape}\")\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EMGFeatureExtractor:\n",
    "    def __init__(self, fs: int = 1000):\n",
    "        self.fs = fs\n",
    "\n",
    "    def bandpass_filter(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply bandpass filter to EMG signals\"\"\"\n",
    "        lowcut = 20\n",
    "        highcut = 500\n",
    "        nyquist = 0.5 * self.fs\n",
    "        low = lowcut / nyquist\n",
    "        high = highcut / nyquist\n",
    "        b, a = butter(4, [low, high], btype='band')\n",
    "        return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "    def extract_time_domain_features(self, signal: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Extract time domain features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Root Mean Square (RMS)\n",
    "        features['rms'] = np.sqrt(np.mean(signal ** 2, axis=0))\n",
    "        \n",
    "        # Mean Absolute Value (MAV)\n",
    "        features['mav'] = np.mean(np.abs(signal), axis=0)\n",
    "        \n",
    "        # Waveform Length (WL)\n",
    "        features['wl'] = np.sum(np.abs(np.diff(signal, axis=0)), axis=0)\n",
    "        \n",
    "        # Zero Crossing Rate (ZCR)\n",
    "        features['zcr'] = np.sum(np.diff(np.signbit(signal), axis=0), axis=0)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def extract_frequency_domain_features(self, signal: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Extract frequency domain features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Power Spectral Density (PSD)\n",
    "        freqs, psd = welch(signal, fs=self.fs, nperseg=256)\n",
    "        features['psd_mean'] = np.mean(psd, axis=0)\n",
    "        features['psd_std'] = np.std(psd, axis=0)\n",
    "        \n",
    "        # Wavelet Transform\n",
    "        coeffs = pywt.wavedec(signal, 'db4', level=4, axis=0)\n",
    "        for i, coeff in enumerate(coeffs):\n",
    "            features[f'wavelet_{i}_mean'] = np.mean(coeff, axis=0)\n",
    "            features[f'wavelet_{i}_std'] = np.std(coeff, axis=0)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def extract_features(self, window: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Extract all features from an EMG window\"\"\"\n",
    "        # Apply bandpass filter\n",
    "        filtered_window = self.bandpass_filter(window)\n",
    "        \n",
    "        # Extract features\n",
    "        time_features = self.extract_time_domain_features(filtered_window)\n",
    "        freq_features = self.extract_frequency_domain_features(filtered_window)\n",
    "        \n",
    "        # Combine features\n",
    "        features = {**time_features, **freq_features}\n",
    "        \n",
    "        # Convert to feature vector\n",
    "        feature_vector = np.concatenate([v.flatten() for v in features.values()])\n",
    "        \n",
    "        return feature_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape: Tuple[int, ...], num_classes: int) -> tf.keras.Model:\n",
    "    \"\"\"Create CNN model\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_shape: Tuple[int, ...], num_classes: int) -> tf.keras.Model:\n",
    "    \"\"\"Create LSTM model\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LSTM(32),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_hybrid_model(input_shape: Tuple[int, ...], num_classes: int) -> tf.keras.Model:\n",
    "    \"\"\"Create hybrid CNN-LSTM model\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # CNN branch\n",
    "    x_cnn = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x_cnn = layers.BatchNormalization()(x_cnn)\n",
    "    x_cnn = layers.MaxPooling1D(pool_size=2)(x_cnn)\n",
    "    x_cnn = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(x_cnn)\n",
    "    x_cnn = layers.BatchNormalization()(x_cnn)\n",
    "    x_cnn = layers.GlobalAveragePooling1D()(x_cnn)\n",
    "    \n",
    "    # LSTM branch\n",
    "    x_lstm = layers.LSTM(64, return_sequences=True)(inputs)\n",
    "    x_lstm = layers.BatchNormalization()(x_lstm)\n",
    "    x_lstm = layers.LSTM(32)(x_lstm)\n",
    "    x_lstm = layers.BatchNormalization()(x_lstm)\n",
    "    \n",
    "    # Combine branches\n",
    "    x = layers.Concatenate()([x_cnn, x_lstm])\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model: tf.keras.Model, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "              X_val: np.ndarray, y_val: np.ndarray,\n",
    "              batch_size: int = 32, epochs: int = 50) -> Dict[str, List[float]]:\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Convert labels to one-hot encoding\n",
    "        y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
    "        y_val_onehot = tf.keras.utils.to_categorical(y_val)\n",
    "        \n",
    "        # Define callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                'best_model.h5',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train_onehot,\n",
    "            validation_data=(X_val, y_val_onehot),\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history.history\n",
    "    \n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_classes),\n",
    "            'f1_score': f1_score(y_test, y_pred_classes, average='weighted'),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred_classes)\n",
    "        }\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def export_model(model: tf.keras.Model, export_dir: str):\n",
    "    \"\"\"Export model to different formats\"\"\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # Save TensorFlow model\n",
    "    model.save(os.path.join(export_dir, 'tf_model'))\n",
    "    \n",
    "    # Export to ONNX\n",
    "    input_signature = (tf.TensorSpec((None,) + model.input_shape[1:], tf.float32),)\n",
    "    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=input_signature)\n",
    "    with open(os.path.join(export_dir, 'model.onnx'), 'wb') as f:\n",
    "        f.write(model_proto.SerializeToString())\n",
    "    \n",
    "    # Export to TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    with open(os.path.join(export_dir, 'model.tflite'), 'wb') as f:\n",
    "        f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " def plot_training_history(history: Dict[str, List[float]], save_path: str):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, save_path: str):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Configuration\n",
    "    DATA_DIR = \"capgmyo_dataset\"\n",
    "    WINDOW_SIZE = 1000\n",
    "    OVERLAP = 0.5\n",
    "    SAMPLING_RATE = 1000\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    os.makedirs('exported_models', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        data_loader = CapgMyoDataLoader(DATA_DIR, WINDOW_SIZE, OVERLAP, SAMPLING_RATE)\n",
    "        data_loader.download_dataset()\n",
    "        X, y = data_loader.load_data()\n",
    "        \n",
    "        # Extract features\n",
    "        feature_extractor = EMGFeatureExtractor(SAMPLING_RATE)\n",
    "        X_features = np.array([feature_extractor.extract_features(window) \n",
    "                             for window in X.reshape(-1, WINDOW_SIZE, X.shape[1])])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_features, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        # Train different models\n",
    "        models_to_train = {\n",
    "            'cnn': create_cnn_model,\n",
    "            'lstm': create_lstm_model,\n",
    "            'hybrid': create_hybrid_model\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for model_name, model_fn in models_to_train.items():\n",
    "            logger.info(f\"Training {model_name} model...\")\n",
    "            \n",
    "            # Create and train model\n",
    "            model = model_fn(X_train.shape[1:], len(np.unique(y)))\n",
    "            trainer = ModelTrainer(model, LEARNING_RATE)\n",
    "            history = trainer.train(X_train, y_train, X_val, y_val, BATCH_SIZE, EPOCHS)\n",
    "            \n",
    "            # Evaluate model\n",
    "            metrics = trainer.evaluate(X_test, y_test)\n",
    "            results[model_name] = {\n",
    "                'history': history,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            \n",
    "            # Plot results\n",
    "            plot_training_history(\n",
    "                history,\n",
    "                f'results/{model_name}_training_history.png'\n",
    "            )\n",
    "            plot_confusion_matrix(\n",
    "                metrics['confusion_matrix'],\n",
    "                f'results/{model_name}_confusion_matrix.png'\n",
    "            )\n",
    "            \n",
    "            # Export model\n",
    "            export_model(model, f'exported_models/{model_name}')\n",
    "            \n",
    "            logger.info(f\"{model_name} model results:\")\n",
    "            logger.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            logger.info(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        # Save all results\n",
    "        import json\n",
    "        with open('results/all_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        logger.info(\"Pipeline completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in pipeline: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
